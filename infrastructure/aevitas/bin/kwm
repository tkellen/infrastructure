#!/usr/bin/env bash
# Kubernetes Without Magic
#
# MIT License
#
# Copyright (c) 2018 Authors of Kubernetes Without Magic
#
# Permission is hereby granted, free of charge, to any person
# obtaining a copy of this software and associated documentation
# files (the "Software"), to deal in the Software without
# restriction, including without limitation the rights to use,
# copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following
# conditions:
#
# The above copyright notice and this permission notice shall be
# included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.

set -euo pipefail

export VERSION="v0.3.1 / e876c42"

# Controls whether or not KWM will look to disk for templates.
export TEMPLATES_BUNDLED=true

# Set some defaults.
export KWM_POD_CIDR=${KWM_POD_CIDR:-10.1.0.0/16}
export KWM_SERVICE_CIDR=${KWM_SERVICE_CIDR:-10.10.0.0/24}
export KWM_KUBERNETES_SERVICE_IP=${KWM_KUBERNETES_SERVICE_IP:-10.10.0.1}
export KWM_DNS_SERVICE_IP=${KWM_DNS_SERVICE_IP:-10.10.0.2}
export KWM_CONFIG_PATH=${KWM_CONFIG_PATH:-/etc/kubernetes}
export KWM_LOCAL_PKI_PATH=${KWM_LOCAL_PKI_PATH:-pki}
export KWM_VERSION_ETCD=${KWM_VERSION_ETCD:-3.2.14}
export KWM_VERSION_KUBERNETES=${KWM_VERSION_KUBERNETES:-1.9.1}
export KWM_VERSION_KUBE_DNS=${KWM_VERSION_KUBE_DNS:-1.14.8}
export KWM_VERSION_CNI_PLUGIN=${KWM_VERSION_CNI_PLUGIN:-0.6.0}
export KWM_VERSION_CRI_CONTAINERD=${KWM_VERSION_CRI_CONTAINERD:-1.0.0-beta.1}
export KWM_VERSION_KUBE_ROUTER=${KWM_VERSION_KUBE_ROUTER:-0.1.0-rc1}

# Get an absolute path to KWM itself.
export BASE_PATH="$(cd "$(dirname "$0")"; pwd -P)/"

# Get the name of KWM, just in case it has been changed.
export SCRIPT_NAME="$(basename -- "$0")"

# Many commands highlight user supplied input with coloring. This variable is
# used to control instances where colors should not be applied. For example,
# a user may run `kwm render pki` to inspect the commands needed to generate
# PKI for Kubernetes. If they actually want to execute those commands, as in
# `kwm render pki | bash`, the highlighting of their values should not be
# present.
if [[ ! -t 1 ]]; then
  export STDOUT_IS_TERMINAL=false
else
  export STDOUT_IS_TERMINAL=true
fi

# A convenience flag that allows users to generate invalid resources by
# providing partially complete environment values. This makes it easy to
# explore where individual enviroment values wind up.
export IGNORE_MISSING_ENV=false
case "$@" in
  *--ignore-missing-env*) export IGNORE_MISSING_ENV=true ;;
esac

# Output supplied string in red color.
error() {
  local message=${1:-""}
  (
    tput setaf 1
    printf "%s" "$message"
    tput op
  ) >&2
}
export -f error # allow subprocesses to access this method

# Find environment variables under nodeKey namespace.
#
# Examples:
# KWM_CONNECT=DEFAULT KWM_CONNECT_node1=TEST nodeValue node1 CONNECT
# ^ returns TEST
# KWM_CONNECT=DEFAULT KWM_CONNECT_node1=TEST nodeValue node2 CONNECT
# ^ returns DEFAULT
# KWM_CONNECT_node1=TEST nodeValue node2 CONNECT
# ^ returns nothing
nodeValue() {
  local nodeKey=${1-:""}
  local var=${2-:""}
  local nodeLookup="KWM_${var}_${nodeKey}"
  local defaultLookup="KWM_${var}"
  local nodeValue=${!nodeLookup:-""}
  local defaultValue=${!defaultLookup:-""}
  printf "%s\n" "${nodeValue:-$defaultValue}"
}
export -f nodeValue # allow subprocesses to access this method

# Render templates from disk or embedded lookup functions.
template() {
  local namespace=${1:-""}
  local key=${2:-""}
  local templateContent
  # Look for requested template on disk
  local templatePath="${BASE_PATH}template/$namespace/$key"
  # Look for inline template function (built, single-file mode)
  local templateFn="template_${namespace}_${key}"
  # When operating in built mode, a lookup function for each namespace
  # exists. If that function can be found, use it to find the template content.
  if [[ $TEMPLATES_BUNDLED == true ]]; then
    templateContent="$($templateFn)"
  else
    templateContent="$(cat "$templatePath")"
  fi
  # If rendering the requested template is blank, bail with error screen.
  if [[ -z $templateContent ]]; then
    # Prevent recursive loop trying to find "resource-not-found" error.
    if [[ $namespace == error ]]; then
      printf "%s\n" "This is pretty bad."
    else
      # Show error stating template couldn't be found. This could cause a
      # recursive loop without the above conditional if the "resource-not-found"
      # template could not be found.
      error "$(missing=$key template error resource-not-found)"
    fi
    exit 1
  fi
  # Render our template and "compile" it by passing it into bash.
  _compile "$(sed 's/\\/\\\\/g' <<<"$templateContent")" | bash
}

# Make a string blue.
header() {
  printf "$(tput setaf 4)$1$(tput op)"
}

# Take a literal string that may contain variables and function calls and drop
# it into a heredoc string for later evaluation by bash.
# This is pretty voodoo :D.
_compile() {
  printf "%b\n" "cat <<RENDER"
  printf "%b\n" "$1"
  printf "%b\n" "RENDER"
}
export -f template header _compile  # allow subprocesses to access these functions

# Connect to a Node by executing the command specified under
# KWM_CONNECT_[nodeKey].
connect() {
  local nodeKey=${1:-""}
  # Look up KWM_CONNECT_[nodeKey]
  local call=$(nodeValue $nodeKey CONNECT)
  # If no node is defined, bail with usage screen.
  [[ -z $nodeKey ]] && template usage connect && exit 1
  # If node cannot be found in environment, bail with a notification.
  [[ -z $call ]] && error "$(requested=$nodeKey template error invalid-node)" && exit 1
  # Execute KWM_CONNECT for the specified node.
  $call
  exit 0
}

# Return a list of in-scope variables that contain the supplied string.
findVars() {
  local search=${1:-""}
  declare -xp | sed 's/declare -x \([^=]*\)=.*/\1/g' | grep "$search"
}

# Display details about a KWM_* environment variable.
define() {
  local var=${1:-""}
  # If no variable is defined, bail with a usage screen.
  [[ -z $var ]] && vars="$(_getDefinable)" template usage define && exit 1
  # Show variable name before definition.
  printf "%s\n" "$var (current value: $(highlight "${!var:-"n/a"}"))"
  # Display definition!
  template define $var
  exit 0
}

# Get a list of possible environment values to be defined. In the development of
# KWM these values are read from disk. In bundled versions of KWM, templates are
# inlined into the script.
_getDefinable() {
  local prefix="template_define_"
  if [[ $VERSION == dev ]]; then
    printf "%s\n" "$(ls ${BASE_PATH}template/define)"
  else
    printf "%s\n" "$(compgen -A function | grep $prefix | sed "s/$prefix//g")"
  fi
}

# Colorize all variables beginning with the string KWM. This makes them easy
# to see in rendered templates.
highlightAll() {
  for name in ${!KWM*}; do
    eval $name=\"$(tput setaf 4)${!name}$(tput op)\"
  done
}

highlight() {
  if $STDOUT_IS_TERMINAL; then
    tput setaf 4
    sed "s/\x1B\[[0-9;]*m//g" <<<"$1"
    tput op
  else
    printf "$1"
  fi
}
export -f highlight

# Find all Node keys specified in the environment.
findNodes() {
  local type=${1:-""}
  local nodeKey
  for hit in $(findVars "KWM_ROLE_.*"); do
    nodeKey="${hit##*_}"
    if [[ $type == all || ${!hit} =~ $type ]]; then
      printf "%s\n" $nodeKey
    fi
  done
}
export -f findNodes # allow subprocesses to access this method

# These "magic" functions somewhat violate the premise of this project and may
# be removed in future revisions.

# Populate etcd values based on configuration for any Node with a KWM_ROLE that
# contains "etcd".
#
# During PKI generation, etcd cluster bootstrapping, and configuring
# kube-apiserver, a full list of IPs and hostnames for etcd Nodes in various
# formats is needed.
magicEtcdMeta() {
  export KWM_ETCD_INITIAL_CLUSTER=${KWM_ETCD_INITIAL_CLUSTER:-$(_magicEtcdInitialCluster)}
  export KWM_ETCD_CLIENT_SANS=${KWM_ETCD_CLIENT_SANS:-$(_magicEtcdClientSans)}
  export KWM_ETCD_SERVERS=${KWM_ETCD_SERVERS:-$(_magicEtcdServers)}
}

# Generate all valid Subject Alternative Names for securing communication from
# the apiserver to etcd.
_magicEtcdClientSans() {
  local output
  for node in $(findNodes etcd); do
    output+=",IP:$(nodeValue $node PRIVATE_IP),DNS:$(nodeValue $node HOSTNAME)"
  done
  [[ -n $output ]] && printf "%s\n" ${output:1}
}

# Generate the value passed to etcd's "--initial-cluster" flag when spinning up
# a new cluster.
_magicEtcdInitialCluster() {
  local output=""
  for node in $(findNodes etcd); do
    output+=",$(nodeValue $node HOSTNAME)=https://$(nodeValue $node PRIVATE_IP):2380"
  done
  [[ -n $output ]] && printf "%s\n" ${output:1}

}

# Generate the value passed to kube-apiserver's "--etcd-servers" flag.
_magicEtcdServers() {
  local output
  for node in $(findNodes etcd); do
    output+=",https://$(nodeValue $node PRIVATE_IP):2379"
  done
  [[ -n $output ]] && printf "%s\n" ${output:1}
}

# These "magic" functions somewhat violate the premise of this project and may
# be removed in future revisions.
# When generating scripts for nodes, try to assign environment values in the
# main KWM namespace based on the provided "node key".
#
# That is, if the user has an environment like this:
# KWM_CONNECT_node1="ssh root@55.55.55.55"
# KWM_PRIVATE_IP_node1="10.100.10.1"
# KWM_HOSTNAME_node1="my-first-node"
#
# KWM_CONNECT_node2="ssh root@44.44.44.44"
# KWM_PRIVATE_IP_node2="10.100.10.2"
# KWM_HOSTNAME_node2="my-second-node"
#
# A provided "node key" of "node1" will merge the values into the main KWM
# namespace, like so:
# KWM_CONNECT="ssh root@55.55.55.55"
# KWM_PRIVATE_IP="10.100.10.1"
# KWM_HOSTNAME="my-first-node"
magicNodeMeta() {
  local nodeKey=${1:-""}
  export KWM_ROLE="${KWM_ROLE:-$(nodeValue $nodeKey ROLE)}"
  export KWM_HOSTNAME="${KWM_HOSTNAME:-$(nodeValue $nodeKey HOSTNAME)}"
  export KWM_CONNECT="${KWM_CONNECT:-$(nodeValue $nodeKey CONNECT)}"
  export KWM_PRIVATE_IP="${KWM_PRIVATE_IP:-$(nodeValue $nodeKey PRIVATE_IP)}"
}

# Enumerate required environment variables for each resource type.
requiredEnv() {
  local key="${1/-/_}"
  local lookup="$key[*]"
  local pki=(
    KWM_CLUSTER_NAME
    KWM_APISERVER_PUBLIC_IP
    KWM_LOCAL_PKI_PATH
    KWM_ETCD_CLIENT_SANS
  )
  local cluster_admin=(
    KWM_CLUSTER_NAME
    KWM_LOCAL_PKI_PATH
    KWM_APISERVER_PUBLIC_IP
  )
  local cni_manifest=(KWM_VERSION_KUBE_ROUTER)
  local controlplane_node=(
    KWM_CONNECT
    KWM_PRIVATE_IP
    KWM_HOSTNAME
    KWM_ROLE
    KWM_APISERVER_PRIVATE_IP
    KWM_CLUSTER_NAME
    KWM_VERSION_KUBERNETES
    KWM_VERSION_CNI_PLUGIN
    KWM_VERSION_CRI_CONTAINERD
    KWM_POD_CIDR
    KWM_SERVICE_CIDR
    KWM_KUBERNETES_SERVICE_IP
    KWM_DNS_SERVICE_IP
    KWM_APISERVER_PUBLIC_IP
    KWM_ETCD_SERVERS
    KWM_LOCAL_PKI_PATH
    KWM_CONFIG_PATH
  )
  local dns_manifest=(KWM_VERSION_KUBE_DNS KWM_DNS_SERVICE_IP)
  local etcd_node=(
    KWM_CONNECT
    KWM_PRIVATE_IP
    KWM_HOSTNAME
    KWM_ETCD_INITIAL_CLUSTER
    KWM_VERSION_ETCD
    KWM_LOCAL_PKI_PATH
    KWM_CONFIG_PATH
  )
  local startup=(
    KWM_CLUSTER_NAME
    KWM_APISERVER_PUBLIC_IP
    KWM_APISERVER_PRIVATE_IP
    KWM_POD_CIDR
    KWM_SERVICE_CIDR
    KWM_KUBERNETES_SERVICE_IP
    KWM_DNS_SERVICE_IP
    KWM_CONFIG_PATH
    KWM_LOCAL_PKI_PATH
    KWM_VERSION_ETCD
    KWM_VERSION_KUBERNETES
    KWM_VERSION_KUBE_DNS
    KWM_VERSION_CNI_PLUGIN
    KWM_VERSION_CRI_CONTAINERD
    KWM_VERSION_KUBE_ROUTER
  )
  local worker_node=(
    KWM_CONNECT
    KWM_PRIVATE_IP
    KWM_HOSTNAME
    KWM_ROLE
    KWM_APISERVER_PRIVATE_IP
    KWM_CLUSTER_NAME
    KWM_VERSION_KUBERNETES
    KWM_VERSION_CNI_PLUGIN
    KWM_VERSION_CRI_CONTAINERD
    KWM_POD_CIDR
    KWM_LOCAL_PKI_PATH
    KWM_CONFIG_PATH
  )
  [[ -z "${!lookup}" ]] && exit 1
  printf "${!lookup}"
}

# Display current values in environment for a specified resource.
getenv() {
  local resource=${1:-""}
  local nodeKey=${2:-""}
  # If no resource is defined, bail with a usage screen.
  [[ -z $resource ]] && template usage env && exit 1
  # Look up required environment variables
  local requiredEnv="$(requiredEnv $resource)"
  # Bail as bad resource if none found.
  [[ -z $requiredEnv ]] && error "$(missing=$resource template error resource-not-found)" && exit 1
  # If the resource type contains the string node...
  if [[ $resource =~ node ]]; then
    # If a nodeKey isn't defined for a node resource, show error.
    [[ -z $nodeKey ]] && error "$(resource=$resource template error no-node-for-env)" && printf '\n\n'
    # Find KWM_*_[nodeKey] values and collapse into root namespace.
    magicNodeMeta $nodeKey
  fi
  # Look at all nodes and build environment variables for etcd.
  magicEtcdMeta
  # If output is bound for a terminal, highlight all KWM_* variables.
  [[ $STDOUT_IS_TERMINAL == true ]] && highlightAll
  # Display all vars needed for the specified resource + their current values.
  for key in $requiredEnv; do
    printf "%s\n" "${key}=\"${!key:-""}\""
  done
  exit 0
}

# Show explanation of what KWM is trying to accomplish.
function help() {
  template usage help
  exit 0
}

# Convenience command to add the directory of this script to PATH.
installer() {
  echo "export PATH=\$PATH:\"${BASE_PATH%%/}\""
  exit 0
}

# List all nodes in the environment filtered by their role.
nodes() {
  local role=${1:-""}
  # If no role defined, bail with usage screen.
  [[ -z $role ]] && template usage nodes && exit 1
  # Find all unique KWM_ROLE_[nodeKey] entries for specifed role
  local nodes="$(findNodes $role)"
  # If none found, bail with error screen.
  [[ -z $nodes ]] && error "$(template error no-nodes-for-role)" && exit 1
  # Print the node keys!
  printf "%s\n" "$nodes"
  exit 0
}

# Render a template for bootstrapping a Kubernetes cluster.
render() {
  local resource=${1:-""}
  local nodeKey=${2:-""}
  # If no resource requested, bail with usage screen.
  [[ -z $resource ]] && template usage render && exit 1
  # Look at all nodes and build environment variables for etcd.
  magicEtcdMeta
  # If the resource type is node, collapse all KWM_*_[nodeKey] values.
  [[ $resource =~ node ]] && magicNodeMeta $nodeKey
  # Look up required environment variables.
  local requiredEnv="$(requiredEnv $resource)"
  # Find those that are missing.
  local missing="$(
    for var in $requiredEnv; do
      [[ -z ${!var:-""} ]] && printf "%s\n" "$var"
    done
  )"
  # Allow skipping validation for the convenience of viewing templates.
  if [[ -n $missing && $IGNORE_MISSING_ENV != true ]]; then
    error "$(missing="$missing" resource="$resource" template error env-missing)"
    exit 1
  fi
  # If output is bound for a terminal, highlight all KWM_* variables.
  [[ $STDOUT_IS_TERMINAL == true ]] && highlightAll
  # Render the template
  template resource ${resource}
  exit 0
}

# Generate a one-shot script for bootstrapping a Kubernetes cluster. This
# calls out extensively to KWM itself.
startup() {
  if [[ "$(which $SCRIPT_NAME)" != "$BASE_PATH$SCRIPT_NAME" ]]; then
    printf "\n"
    error "$(template usage install)"
    exit 1
  fi
  # give user something useful to go on if no valid nodes are found.
  # at least one for each role is needed.
  if [[ -z "$(findNodes etcd)$(findNodes controlplane)$(findNodes worker)" ]]; then
    error "$(template error startup-no-nodes)"
    exit 1
  fi
  render startup
  exit 0
}

# Generate commands needed to unset all KWM_* values.
unsetter() {
  # If output is bound for a terminal, show usage screen.
  if [[ $STDOUT_IS_TERMINAL == true ]]; then
    template usage unset
    exit 1
  fi
  # ...otherwise list unset calls for all KWM environment variables.
  findVars "^KWM" | xargs -n 1 echo unset
  exit 0
}

# Show main usage screen and prompt for installation if needed.
usage() {
  template usage main
  if [[ "$(which $SCRIPT_NAME)" != "$BASE_PATH$SCRIPT_NAME" ]]; then
    printf "\n"
    error "$(template usage install)"
    exit 1
  fi
  exit 0
}

# Show version. Good thing this is commented!
version() {
  echo $VERSION;
  exit 0
}

# Display a workshop.
workshop() {
  local name=${1:-""}
  # If no variable is defined, bail with a usage screen.
  if [[ -z $name ]]; then
    template usage workshop
    exit 1
  fi
  printf "%s\n\n" "${name/-/ }" | tr '[:lower:]' '[:upper:]'
  template workshop ${name}
  exit 0
}

# Wire command line arguments to method calls in the script.
main() {
  local command=${1:-usage}
  if [[ $command == connect ]]; then connect ${@:2}; fi
  if [[ $command == define ]]; then define ${@:2}; fi
  if [[ $command == env ]]; then getenv ${@:2}; fi
  if [[ $command == help ]]; then help ${@:2}; fi
  if [[ $command == nodes ]]; then nodes ${@:2}; fi
  if [[ $command == install ]]; then installer; fi
  if [[ $command == render ]]; then render ${@:2}; fi
  if [[ $command == startup ]]; then startup; fi
  if [[ $command == unset ]]; then unsetter; fi
  if [[ $command == version ]]; then version; fi
  if [[ $command == workshop ]]; then workshop ${@:2}; fi
  usage
}

template_define_KWM_APISERVER_PRIVATE_IP() {
cat <<'TEMPLATE'
Kubelets communicate with kube-apiserver using this value.

See \$KWM_CONFIG_PATH/kubelet.kubeconfig on any worker node.

# TODO: ensure there is HA support for this.
# See how this variable is used in defining configuration for kubectl.
KWM_CLUSTER_NAME=test KWM_LOCAL_PKI_PATH=pki $SCRIPT_NAME render cluster-admin
KWM_CLUSTER_NAME=test KWM_LOCAL_PKI_PATH=pki $SCRIPT_NAME render cluster-admin | grep test
TEMPLATE
}
export -f template_define_KWM_APISERVER_PRIVATE_IP

template_define_KWM_APISERVER_PUBLIC_IP() {
cat <<'TEMPLATE'
Kubectl communicates with kube-apiserver using this value.

Examples:
  # See how this variable is used in defining configuration for kubectl.
  KWM_APISERVER_PUBLIC_IP=55.55.55.55 KWM_CLUSTER_NAME=test KWM_LOCAL_PKI_PATH=pki $SCRIPT_NAME render cluster-admin
  KWM_APISERVER_PUBLIC_IP=55.55.55.55 KWM_CLUSTER_NAME=test KWM_LOCAL_PKI_PATH=pki $SCRIPT_NAME render cluster-admin | grep "55.55.55.55"
TEMPLATE
}
export -f template_define_KWM_APISERVER_PUBLIC_IP

template_define_KWM_CLUSTER_NAME() {
cat <<'TEMPLATE'
Used in places where cluster name can provide meaningful context.

Example:
  # See how this variable is used in defining configuration for kubectl.
  KWM_APISERVER_PUBLIC_IP=55.55.55.55 KWM_CLUSTER_NAME=test KWM_LOCAL_PKI_PATH=pki $SCRIPT_NAME render cluster-admin
  KWM_APISERVER_PUBLIC_IP=55.55.55.55 KWM_CLUSTER_NAME=test KWM_LOCAL_PKI_PATH=pki $SCRIPT_NAME render cluster-admin | grep test

  # See how this variable is used in defining common names for pki.
  KWM_APISERVER_PUBLIC_IP=55.55.55.55 KWM_CLUSTER_NAME=test KWM_LOCAL_PKI_PATH=pki KWM_ETCD_CLIENT_SANS=junk $SCRIPT_NAME render pki
  KWM_APISERVER_PUBLIC_IP=55.55.55.55 KWM_CLUSTER_NAME=test KWM_LOCAL_PKI_PATH=pki KWM_ETCD_CLIENT_SANS=junk $SCRIPT_NAME render pki | grep test
TEMPLATE
}
export -f template_define_KWM_CLUSTER_NAME

template_define_KWM_CONFIG_PATH() {
cat <<'TEMPLATE'
Controls where PKI and configuration files will be placed on Nodes during
installation.
TEMPLATE
}
export -f template_define_KWM_CONFIG_PATH

template_define_KWM_CONNECT() {
cat <<'TEMPLATE'
A command to connect to your Node.

Examples:
  # If your Node is directly reachable by the machine running kwm.
  KWM_CONNECT="ssh root@55.55.55.55"

  # If your Node must be accessed through a bastion host.
  KWM_CONNECT="ssh -A -t root@44.44.44.44 ssh -A root@55.55.55.55"

  # If you don't care about trust-on-first-use.
  KWM_CONNECT="ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no root@55.55.55.55"

Notes:
  Be sure to quote this environment variable or things will get strange fast.
TEMPLATE
}
export -f template_define_KWM_CONNECT

template_define_KWM_DNS_SERVICE_IP() {
cat <<'TEMPLATE'
Ensures cluster-aware DNS is available at a known IP within the cluster.

The "clusterIP" for the kube-dns Service spec uses this value.

The "--cluster-dns" command line flag for Kubelets use this value. Every
container in the cluster will have this value placed in /etc/resolv.conf.
TEMPLATE
}
export -f template_define_KWM_DNS_SERVICE_IP

template_define_KWM_ETCD_CLIENT_SANS() {
cat <<'TEMPLATE'
Supplied as the valid Subject Alternative Names to openssl when generating the
certificate for securing communication from the apiserver to etcd.

If not specified explicitly, this is "magically" produced by using all hosts in
the environment with a role of etcd.

For example, given the following hosts in the environment:

KWM_ROLE_node0=etcd
KWM_HOSTNAME_node0=etcd0
KWM_PRIVATE_IP_node0=10.100.0.5
...
KWM_ROLE_node1=etcd
KWM_HOSTNAME_node1=etcd1
KWM_PRIVATE_IP_node1=10.100.1.5
...
KWM_ROLE_node2=etcd
KWM_HOSTNAME_node2=etcd2
KWM_PRIVATE_IP_node0=10.100.2.5

...the resultant value would be:
KWM_ETCD_CLIENT_SANS=IP:10.100.0.5,DNS:etcd0,IP:10.100.1.5,DNS:etcd1,IP:10.100.2.5,DNS:etcd2

Examples:
  # Inspect this value by looking at the controlplane node installation script.
  $SCRIPT_NAME render controlplane-node <nodeKey>
TEMPLATE
}
export -f template_define_KWM_ETCD_CLIENT_SANS

template_define_KWM_ETCD_INITIAL_CLUSTER() {
cat <<'TEMPLATE'
Supplied as the "--initial-cluster" value to etcd for bootstrapping the cluster.

If not specified explicitly, this is "magically" produced using all hosts in the
environment with a role of etcd.

For example, given the following hosts in the environment:

KWM_ROLE_node0=etcd
KWM_HOSTNAME_node0=etcd0
KWM_PRIVATE_IP_node0=10.100.0.5
...
KWM_ROLE_node1=etcd
KWM_HOSTNAME_node1=etcd1
KWM_PRIVATE_IP_node1=10.100.1.5
...
KWM_ROLE_node2=etcd
KWM_HOSTNAME_node2=etcd2
KWM_PRIVATE_IP_node0=10.100.2.5

...the resultant value would be:
KWM_ETCD_INITIAL_CLUSTER=aws-etcd-0=https://10.100.0.5:2380,aws-etcd-1=https://10.100.1.5:2380,aws-etcd-2=https://10.100.2.5:2380

Examples:
  # Inspect this value by looking at the etcd node installation script.
  $SCRIPT_NAME render etcd-node <nodeKey>
TEMPLATE
}
export -f template_define_KWM_ETCD_INITIAL_CLUSTER

template_define_KWM_ETCD_SERVERS() {
cat <<'TEMPLATE'
Supplied as the "--etcd-servers" value to kube-apiserver.

If not specified explicitly, this is "magically" produced by using all hosts in
the environment with a role of etcd.

For example, given the following hosts in the environment:

KWM_ROLE_node0=etcd
KWM_PRIVATE_IP_node0=10.100.0.5
...
KWM_ROLE_node1=etcd
KWM_PRIVATE_IP_node1=10.100.1.5
...
KWM_ROLE_node2=etcd
KWM_PRIVATE_IP_node0=10.100.2.5

...the resultant value would be:
https://10.100.0.5:2379,https://10.100.1.5:2379,https://10.100.2.5:2379

Examples:
  # Inspect this value by looking at pki generation.
  $SCRIPT_NAME render pki

  # Inspect this value by reading the generated cert directly.
  openssl x509 -noout -text -in apiserver-to-etcd-cert.pem | grep DNS
TEMPLATE
}
export -f template_define_KWM_ETCD_SERVERS

template_define_KWM_HOSTNAME() {
cat <<'TEMPLATE'
Used during the installation of any node to ensure the machine hostname is set
correctly and a loopback reference is available in /etc/hosts.

Examples:
  # Inspect this value by looking at the node installation scripts.
  $SCRIPT_NAME render etcd-node <nodeKey>
  $SCRIPT_NAME render controlplane-node <nodeKey>
  $SCRIPT_NAME render worker-node <nodeKey>
TEMPLATE
}
export -f template_define_KWM_HOSTNAME

template_define_KWM_KUBERNETES_SERVICE_IP() {
cat <<'TEMPLATE'
The IP used for kubernetes.default.svc.cluster.local
TEMPLATE
}
export -f template_define_KWM_KUBERNETES_SERVICE_IP

template_define_KWM_LOCAL_PKI_PATH() {
cat <<'TEMPLATE'
Controls both where PKI will be generated, and, when securely copied to Nodes,
during bootstrapping, where it will be sourced from.

Examples:
  # See how this value affects PKI generation.
  $SCRIPT_NAME render pki

  # See how this value affects copying PKI.
  $SCRIPT_NAME render etcd-node <nodeKey> | head -n 10
TEMPLATE
}
export -f template_define_KWM_LOCAL_PKI_PATH

template_define_KWM_POD_CIDR() {
cat <<'TEMPLATE'
The network used by all Pods. Used by kubelet, kube-controller-manager and
kube-router.

TODO: define further
TEMPLATE
}
export -f template_define_KWM_POD_CIDR

template_define_KWM_PRIVATE_IP() {
cat <<'TEMPLATE'
The private IP of a node. Used for configuring internal communication between
Kubernetes components.

Examples:
  # Show how this is used to configure etcd nodes.
  KWM_PRIVATE_IP=10.100.0.1 $SCRIPT_NAME render etcd-node --ignore-missing-env

  KWM_CONNECT="dummy" KWM_HOSTNAME="host" KWM_PRIVATE_IP=10.100.0.1 $SCRIPT_NAME render etcd-node

Notes:
  Be sure to quote this environment variable or things will get strange fast.
TEMPLATE
}
export -f template_define_KWM_PRIVATE_IP

template_define_KWM_ROLE() {
cat <<'TEMPLATE'
Controls the role a node plays in the cluster. Multiple roles can be applied by
separating them with spaces, e.g.

  etcd
    Runs etcd.

  controlplane
    Runs kube-apiserver, kube-scheduler and kube-controller-manager.

  worker
    Runs kubelet, containerd, cri-containerd.

For nodes marked as a worker, this controls the role applied to the Kubelet.

For example:

KWM_ROLE_nodeKey="controlplane worker"

...will add these flags to Kubelet:

  --node-labels="node-role.kubernetes.io/controlplane=true,node-role.kubernetes.io/worker=true"
  --register-with-taints="note-role.kubernetes.io/controlplane=true:NoSchedule"
TEMPLATE
}
export -f template_define_KWM_ROLE

template_define_KWM_SERVICE_CIDR() {
cat <<'TEMPLATE'
The network used by all Services.

TODO: define further
TEMPLATE
}
export -f template_define_KWM_SERVICE_CIDR

template_define_KWM_VERSION_CNI_PLUGIN() {
cat <<'TEMPLATE'
Controls what version of the cni plugins are installed on all Nodes.

https://github.com/containernetworking/cni/releases
TEMPLATE
}
export -f template_define_KWM_VERSION_CNI_PLUGIN

template_define_KWM_VERSION_CRI_CONTAINERD() {
cat <<'TEMPLATE'
Controls what version of cri-containerd is installed on all Nodes.

https://github.com/kubernetes-incubator/cri-containerd/releases
TEMPLATE
}
export -f template_define_KWM_VERSION_CRI_CONTAINERD

template_define_KWM_VERSION_ETCD() {
cat <<'TEMPLATE'
Controls what version of etcd will be installed on Nodes with a KWM_ROLE that
includes etcd.

https://github.com/coreos/etcd/releases/
TEMPLATE
}
export -f template_define_KWM_VERSION_ETCD

template_define_KWM_VERSION_KUBERNETES() {
cat <<'TEMPLATE'
Controls what version of these binaries are installed on Nodes:

  kube-apiserver
  kube-controller-mananger
  kube-scheduler
  kubelet
TEMPLATE
}
export -f template_define_KWM_VERSION_KUBERNETES

template_define_KWM_VERSION_KUBE_DNS() {
cat <<'TEMPLATE'
Controls what container image version is used for kube-dns Pods.

https://github.com/kubernetes/dns/releases
TEMPLATE
}
export -f template_define_KWM_VERSION_KUBE_DNS

template_define_KWM_VERSION_KUBE_ROUTER() {
cat <<'TEMPLATE'
Controls what container image version is used for kube-router Pods.

https://github.com/cloudnativelabs/kube-router/releases
TEMPLATE
}
export -f template_define_KWM_VERSION_KUBE_ROUTER

template_error_env-missing() {
cat <<'TEMPLATE'
Sorry, cannot render ${resource} yet.

Please provide values for these variables:

$(sed 's/^/  /' <<<"${missing:-None}")

Notes:
  Run "kwm env ${resource}" to see all required variables and their current values.
  Run "kwm define <variable>" for more information about the listed variables.
  Run "kwm render ${resource} --ignore-missing-env" to render anyway.
TEMPLATE
}
export -f template_error_env-missing

template_error_invalid-node() {
cat <<'TEMPLATE'
Sorry, unable to locate "${requested}" (KWM_CONNECT_${requested}) in your environment.

You can view the nodes present in your environment with the following:

  $SCRIPT_NAME nodes all
TEMPLATE
}
export -f template_error_invalid-node

template_error_no-node-for-env() {
cat <<'TEMPLATE'
No node defined to generate environment for. Only displaying shared values.``

Notes:
  Check for available nodes by running "kwm nodes ${resource%%-node}"
  Then, specify a node key by running "kwm env ${resource} <nodeKey>"
TEMPLATE
}
export -f template_error_no-node-for-env

template_error_no-nodes-for-role() {
cat <<'TEMPLATE'
No nodes found with the specified role.

Check your environment with the following:

  env | grep "KWM_ROLE_.*"
TEMPLATE
}
export -f template_error_no-nodes-for-role

template_error_resource-not-found() {
cat <<'TEMPLATE'
Error: resource "${missing}" not found.
TEMPLATE
}
export -f template_error_resource-not-found

template_error_startup-no-nodes() {
cat <<'TEMPLATE'
Heads up, if you haven't run "kwm help" at least once, consider doing that now.

Almost there! If you want to use the startup script to install Kubernetes with
a single command, you'll need to include configuration for at least one node in
each role (etcd, controlplane and worker).

If you're testing this for the first time, try defining a single node that
performs all roles. That's not a good plan for production, but it is great
for getting things running quickly to play around.

Here's some boilerplate to get you started:

export KWM_CLUSTER_NAME=kwm
export KWM_APISERVER_PUBLIC_IP=[public-ip-of-only-node]
export KWM_APISERVER_PRIVATE_IP=[private-ip-of-only-node]

export KWM_ROLE_now="etcd controlplane worker"
export KWM_HOSTNAME_now=tiny
export KWM_PRIVATE_IP_now=[your-private-ip]
export KWM_CONNECT_now="ssh [your-sudo-capable-user]@[your-ssh-accessible-ip]"
^
KWM calls the "_now" portion of these variables a "node key". By using different
keys you can define multiple hosts for KWM to manage simultaneously. The value
of they keys is entirely up to you, they have no impact on your deployments.

To see which nodes are available for management, try running "kwm nodes".

--

Once you have everything defined, "kwm startup" will output a bash script that
will bootstrap your cluster. After inspecting it, you can run it thusly:

  $SCRIPT_NAME startup | bash

Good luck!

Note:
  Run "kwm define <variable>" for more information about the listed variables.
TEMPLATE
}
export -f template_error_startup-no-nodes

template_installer_cni-plugins() {
cat <<'TEMPLATE'
cd /tmp
curl -sSLo cni.tgz https://github.com/containernetworking/plugins/releases/download/v${version}/cni-plugins-amd64-v${version}.tgz
mkdir -p /opt/cni/bin /etc/cni/net.d
tar xf cni.tgz --directory /opt/cni/bin
TEMPLATE
}
export -f template_installer_cni-plugins

template_installer_cri-containerd() {
cat <<'TEMPLATE'
cd /tmp
curl -sSLo cri-containerd.tar.gz https://github.com/kubernetes-incubator/cri-containerd/releases/download/v${version}/cri-containerd-${version}.linux-amd64.tar.gz
tar xf cri-containerd.tar.gz --directory /
TEMPLATE
}
export -f template_installer_cri-containerd

template_installer_etcd() {
cat <<'TEMPLATE'
cd /tmp
mkdir -p /var/lib/etcd
curl -sSLo etcd.tar.gz https://github.com/coreos/etcd/releases/download/v${version}/etcd-v${version}-linux-amd64.tar.gz
tar xf etcd.tar.gz
mkdir -p /usr/local/bin
mv etcd-v${version}-linux-amd64/etcd* /usr/local/bin
TEMPLATE
}
export -f template_installer_etcd

template_installer_k8s() {
cat <<'TEMPLATE'
cd /tmp
curl -sSLo ${name} https://storage.googleapis.com/kubernetes-release/release/v${version}/bin/linux/amd64/${name}
chmod +x ${name}
mkdir -p /usr/local/bin
mv ${name} /usr/local/bin
TEMPLATE
}
export -f template_installer_k8s

template_installer_socat() {
cat <<'TEMPLATE'
command -v apt-get &>/dev/null && (apt-get -qq update && apt-get -yqq install socat)
command -v yum  &>/dev/null && yum install socat -y
TEMPLATE
}
export -f template_installer_socat

template_pki_init-ca() {
cat <<'TEMPLATE'
openssl ecparam -name secp521r1 -genkey -noout -out ${basePath}/${name}-ca-private-key.pem
openssl req -x509 -new -sha256 -nodes -days 3650 \\
  -key ${basePath}/${name}-ca-private-key.pem \\
  -out ${basePath}/${name}-ca-cert.pem \\
  -subj "${subj}" \\
  -extensions ext \\
  -config <(printf "
[req]
distinguished_name = default
[default]
[ext]
basicConstraints = critical, CA:TRUE
keyUsage = critical, digitalSignature, keyEncipherment, keyCertSign
")
TEMPLATE
}
export -f template_pki_init-ca

template_pki_private-key() {
cat <<'TEMPLATE'
openssl ecparam -name secp521r1 -genkey -noout -out ${basePath}/${name}-private-key.pem
TEMPLATE
}
export -f template_pki_private-key

template_pki_public-key() {
cat <<'TEMPLATE'
openssl ec -in ${basePath}/${name}-private-key.pem -outform PEM -pubout -out ${basePath}/${name}-public-key.pem
TEMPLATE
}
export -f template_pki_public-key

template_pki_signed-cert() {
cat <<'TEMPLATE'
openssl req -new -sha256 -key ${basePath}/${name}-private-key.pem -subj "${subj}" | \\
  openssl x509 -req -sha256 \\
    -CA ${basePath}/${ca}-ca-cert.pem \\
    -CAkey ${basePath}/${ca}-ca-private-key.pem \\
    -CAcreateserial \\
    -out ${basePath}/${name}-cert.pem \\
    -days 3650 \\
    -extensions ext \\
    -extfile <(printf "
[req]
distinguished_name = default
[default]
[ext]
basicConstraints = critical, CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth, clientAuth
${ext}
")
TEMPLATE
}
export -f template_pki_signed-cert

template_resource_cluster-admin() {
cat <<'TEMPLATE'
#------------------------------------------------------------------------------#
echo Configuring kubectl for administrative access to cluster.
#------------------------------------------------------------------------------#
kubectl config set-cluster ${KWM_CLUSTER_NAME} \\
  --certificate-authority=${KWM_LOCAL_PKI_PATH}/cluster-ca-cert.pem \\
  --embed-certs=true \\
  --server=https://${KWM_APISERVER_PUBLIC_IP}:6443

kubectl config set-credentials ${KWM_CLUSTER_NAME}-root \\
  --client-certificate=${KWM_LOCAL_PKI_PATH}/cluster-admin-to-apiserver-cert.pem \\
  --client-key=${KWM_LOCAL_PKI_PATH}/cluster-admin-to-apiserver-private-key.pem \\
  --embed-certs=true

kubectl config set-context ${KWM_CLUSTER_NAME} \\
  --cluster=${KWM_CLUSTER_NAME} \\
  --user=${KWM_CLUSTER_NAME}-root

kubectl config use-context ${KWM_CLUSTER_NAME}
TEMPLATE
}
export -f template_resource_cluster-admin

template_resource_cni-manifest() {
cat <<'TEMPLATE'
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-router-cfg
  namespace: kube-system
  labels:
    tier: node
    k8s-app: kube-router
data:
  cni-conf.json: |
    {
      "name":"kubernetes",
      "type":"bridge",
      "bridge":"kube-bridge",
      "isDefaultGateway":true,
      "ipam": {
        "type":"host-local"
      }
    }
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-router
  namespace: kube-system
  labels:
    k8s-app: kube-router
spec:
  template:
    metadata:
      labels:
        k8s-app: kube-router
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      containers:
      - name: kube-router
        image: cloudnativelabs/kube-router:v${KWM_VERSION_KUBE_ROUTER}
        args: ["--run-router=true", "--run-firewall=true", "--run-service-proxy=true", "--kubeconfig=/etc/kubernetes/kube-router.kubeconfig"]
        securityContext:
          privileged: true
        imagePullPolicy: Always
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        volumeMounts:
        - name: lib-modules
          mountPath: /lib/modules
          readOnly: true
        - name: cni-conf-dir
          mountPath: /etc/cni/net.d
        - name: kubeconfig
          mountPath: /etc/kubernetes
          readOnly: true
      initContainers:
      - name: install-cni
        image: busybox
        imagePullPolicy: Always
        command:
        - /bin/sh
        - -c
        - set -e -x;
          if [ ! -f /etc/cni/net.d/10-kuberouter.conf ]; then
            TMP=/etc/cni/net.d/.tmp-kuberouter-cfg;
            cp /etc/kube-router/cni-conf.json ${TMP};
            mv ${TMP} /etc/cni/net.d/10-kuberouter.conf;
          fi
        volumeMounts:
        - name: cni-conf-dir
          mountPath: /etc/cni/net.d
        - name: kube-router-cfg
          mountPath: /etc/kube-router
      hostNetwork: true
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoSchedule
        key: node-role.kubernetes.io/controlplane
        operator: Exists
      volumes:
      - name: lib-modules
        hostPath:
          path: /lib/modules
      - name: cni-conf-dir
        hostPath:
          path: /etc/cni/net.d
      - name: kube-router-cfg
        configMap:
          name: kube-router-cfg
      - name: kubeconfig
        hostPath:
          path: /etc/kubernetes
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: null
  name: kube-router-access
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: kube-router
TEMPLATE
}
export -f template_resource_cni-manifest

template_resource_controlplane-node() {
cat <<'TEMPLATE'
#------------------------------------------------------------------------------#
echo Sending shared PKI to $KWM_HOSTNAME.
#------------------------------------------------------------------------------#
$(files="cluster-ca-cert.pem cluster-ca-private-key.pem etcd-ca-cert.pem etcd-ca-private-key.pem apiserver-to-etcd-cert.pem apiserver-to-etcd-private-key.pem service-account-private-key.pem service-account-public-key.pem" \
  exec="$KWM_CONNECT" \
  basePath="$KWM_CONFIG_PATH" \
  sourcePath="$KWM_LOCAL_PKI_PATH" template util tar-copy
)
#------------------------------------------------------------------------------#
echo Installing $KWM_HOSTNAME.
#------------------------------------------------------------------------------#
$(exec="$KWM_CONNECT" content="
#------------------------------------------------------------------------------#
echo Generating private key for client to apiserver communication.
#------------------------------------------------------------------------------#
$(basePath="$KWM_CONFIG_PATH" name="client-to-apiserver" template pki private-key)
#------------------------------------------------------------------------------#
echo Generating private key for apiserver to kubelet communication.
#------------------------------------------------------------------------------#
$(basePath="$KWM_CONFIG_PATH" name="apiserver-to-kubelet" template pki private-key)
#------------------------------------------------------------------------------#
echo Generating cluster-ca signed certificate for client to apiserver communication.
#------------------------------------------------------------------------------#
$(name="client-to-apiserver" \
  basePath="$KWM_CONFIG_PATH" \
  subj="/CN=kube-apiserver" \
  ext="subjectAltName = IP:$KWM_APISERVER_PUBLIC_IP,IP:$KWM_KUBERNETES_SERVICE_IP,IP:$KWM_PRIVATE_IP,DNS:$KWM_HOSTNAME,DNS:kubernetes,DNS:kubernetes.default,DNS:kubernetes.default.svc,DNS:kubernetes.default.svc.cluster,DNS:kubernetes.default.svc.cluster.local" \
  ca="cluster" template pki signed-cert
)
#------------------------------------------------------------------------------#
echo Generating cluster-ca signed certificate for apiserver to kubelet communication.
#------------------------------------------------------------------------------#
$(name="apiserver-to-kubelet" \
  basePath="$KWM_CONFIG_PATH" \
  subj="/CN=kube-apiserver-client/O=system:masters" \
  ca="cluster" template pki signed-cert
)
#------------------------------------------------------------------------------#
echo Ensuring hostname and loopback reference are set.
#------------------------------------------------------------------------------#
$(name="$KWM_HOSTNAME" template util set-hostname)
#------------------------------------------------------------------------------#
echo Installing kube-apiserver at version $KWM_VERSION_KUBERNETES.
#------------------------------------------------------------------------------#
$(version="$KWM_VERSION_KUBERNETES" name="kube-apiserver" template installer k8s)
#------------------------------------------------------------------------------#
echo Generating kube-apiserver service file.
#------------------------------------------------------------------------------#
$(path="/etc/systemd/system/kube-apiserver.service" content="$(
  name="kube-apiserver" \
  after="network.target" \
  exec="$(
    count="1" \
    privateIp="$KWM_PRIVATE_IP" \
    serviceCidr="$KWM_SERVICE_CIDR" \
    etcdServers="$KWM_ETCD_SERVERS" template service kube-apiserver
  )" template service unit)" template util write-file
)
#------------------------------------------------------------------------------#
echo Enabling kube-apiserver service.
#------------------------------------------------------------------------------#
$(name="kube-apiserver" template service enable)
#------------------------------------------------------------------------------#
echo Installing kube-controller-manager at version $KWM_VERSION_KUBERNETES.
#------------------------------------------------------------------------------#
$(version="$KWM_VERSION_KUBERNETES" name="kube-controller-manager" template installer k8s)
#------------------------------------------------------------------------------#
echo Generating kube-controller service file.
#------------------------------------------------------------------------------#
$(path="/etc/systemd/system/kube-controller-manager.service" content="$(
  name="kube-controller-manager" \
  after="network.target" \
  requires="" \
  exec="$(
    clusterName="$KWM_CLUSTER_NAME" \
    serviceCidr="$KWM_SERVICE_CIDR" \
    podCidr="$KWM_POD_CIDR" \
    template service kube-controller-manager
  )" template service unit)" template util write-file
)
#------------------------------------------------------------------------------#
echo Enabling kube-controller-manager service.
#------------------------------------------------------------------------------#
$(name="kube-controller-manager" template service enable)
#------------------------------------------------------------------------------#
echo Installing kube-scheduler at version $KWM_VERSION_KUBERNETES.
#------------------------------------------------------------------------------#
$(version="$KWM_VERSION_KUBERNETES" name="kube-scheduler" template installer k8s)
#------------------------------------------------------------------------------#
echo Generating kube-scheduler service file.
#------------------------------------------------------------------------------#
$(path="/etc/systemd/system/kube-scheduler.service" content="$(
  name="kube-scheduler" \
  after="network.target" \
  requires="" \
  exec="$(template service kube-scheduler)" template service unit)" template util write-file
)
#------------------------------------------------------------------------------#
echo Enabling kube-scheduler service.
#------------------------------------------------------------------------------#
$(version="$KWM_VERSION_KUBERNETES" name="kube-scheduler" template service enable)
#------------------------------------------------------------------------------#
echo Installing kubectl at version $KWM_VERSION_KUBERNETES.
#------------------------------------------------------------------------------#
$(version="$KWM_VERSION_KUBERNETES" name="kubectl" template installer k8s)

" template util remote-script)
TEMPLATE
}
export -f template_resource_controlplane-node

template_resource_dns-manifest() {
cat <<'TEMPLATE'
# Copyright 2016 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Should keep target in cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
# in sync with this file.

# Warning: This is a file generated from the base underscore template file: kube-dns.yaml.base

apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "KubeDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: ${KWM_DNS_SERVICE_IP}
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  # replicas: not specified here:
  # 1. In order to make Addon Manager do not reconcile this replicas parameter.
  # 2. Default is 1.
  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 0
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      nodeSelector:
        node-role.kubernetes.io/worker: "true"
      tolerations:
      - key: "CriticalAddonsOnly"
        operator: "Exists"
      volumes:
      - name: kube-dns-config
        configMap:
          name: kube-dns
          optional: true
      containers:
      - name: kubedns
        image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:${KWM_VERSION_KUBE_DNS}
        resources:
          # TODO: Set memory limits when we've profiled the container for large
          # clusters, then set request = limit to keep this container in
          # guaranteed class. Currently, this container falls into the
          # "burstable" category so the kubelet doesn't backoff from restarting it.
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        livenessProbe:
          httpGet:
            path: /healthcheck/kubedns
            port: 10054
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /readiness
            port: 8081
            scheme: HTTP
          # we poll on pod startup for the Kubernetes master service and
          # only setup the /readiness HTTP server once that's available.
          initialDelaySeconds: 3
          timeoutSeconds: 5
        args:
        - --domain=cluster.local.
        - --dns-port=10053
        - --config-dir=/kube-dns-config
        - --v=2
        env:
        - name: PROMETHEUS_PORT
          value: "10055"
        ports:
        - containerPort: 10053
          name: dns-local
          protocol: UDP
        - containerPort: 10053
          name: dns-tcp-local
          protocol: TCP
        - containerPort: 10055
          name: metrics
          protocol: TCP
        volumeMounts:
        - name: kube-dns-config
          mountPath: /kube-dns-config
      - name: dnsmasq
        image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:${KWM_VERSION_KUBE_DNS}
        livenessProbe:
          httpGet:
            path: /healthcheck/dnsmasq
            port: 10054
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        args:
        - -v=2
        - -logtostderr
        - -configDir=/etc/k8s/dns/dnsmasq-nanny
        - -restartDnsmasq=true
        - --
        - -k
        - --cache-size=1000
        - --no-negcache
        - --log-facility=-
        - --server=/cluster.local/127.0.0.1#10053
        - --server=/in-addr.arpa/127.0.0.1#10053
        - --server=/ip6.arpa/127.0.0.1#10053
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        # see: https://github.com/kubernetes/kubernetes/issues/29055 for details
        resources:
          requests:
            cpu: 150m
            memory: 20Mi
        volumeMounts:
        - name: kube-dns-config
          mountPath: /etc/k8s/dns/dnsmasq-nanny
      - name: sidecar
        image: gcr.io/google_containers/k8s-dns-sidecar-amd64:${KWM_VERSION_KUBE_DNS}
        livenessProbe:
          httpGet:
            path: /metrics
            port: 10054
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        args:
        - --v=2
        - --logtostderr
        - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
        - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
        ports:
        - containerPort: 10054
          name: metrics
          protocol: TCP
        resources:
          requests:
            memory: 20Mi
            cpu: 10m
      dnsPolicy: Default  # Don't use cluster DNS.
      serviceAccountName: kube-dns
TEMPLATE
}
export -f template_resource_dns-manifest

template_resource_etcd-node() {
cat <<'TEMPLATE'
#------------------------------------------------------------------------------#
echo Sending shared PKI to $KWM_HOSTNAME.
#------------------------------------------------------------------------------#
$(files="etcd-ca-cert.pem etcd-ca-private-key.pem apiserver-to-etcd-private-key.pem apiserver-to-etcd-cert.pem" \
  exec="$KWM_CONNECT" \
  basePath="$KWM_CONFIG_PATH" \
  sourcePath="$KWM_LOCAL_PKI_PATH" template util tar-copy
)
#------------------------------------------------------------------------------#
echo Installing $KWM_HOSTNAME.
#------------------------------------------------------------------------------#
$(exec="$KWM_CONNECT" content="
#------------------------------------------------------------------------------#
echo Generating private key for etcd server to etcd server communication.
#------------------------------------------------------------------------------#
$(basePath="$KWM_CONFIG_PATH" name="etcd-to-etcd" template pki private-key)
#------------------------------------------------------------------------------#
echo Generating etcd-ca signed certificate for etcd server to etcd server communication.
#------------------------------------------------------------------------------#
$(name="etcd-to-etcd" \
  basePath="$KWM_CONFIG_PATH" \
  subj="/CN=etcd-to-etcd/O=etcd" \
  ext="subjectAltName = IP:$KWM_PRIVATE_IP,DNS:$KWM_HOSTNAME" \
  ca="etcd" template pki signed-cert)
#------------------------------------------------------------------------------#
echo Ensuring hostname and loopback reference are set.
#------------------------------------------------------------------------------#
$(name="$KWM_HOSTNAME" template util set-hostname)
#------------------------------------------------------------------------------#
echo Installing etcd at version $KWM_VERSION_ETCD.
#------------------------------------------------------------------------------#
$(version="$KWM_VERSION_ETCD" template installer etcd)
#------------------------------------------------------------------------------#
echo Generating etcd service file.
#------------------------------------------------------------------------------#
$(path="/etc/systemd/system/etcd.service" content="$(
  name="etcd" \
  after="network.target" \
  exec="$(
    name="$KWM_HOSTNAME" \
    privateIp="$KWM_PRIVATE_IP" \
    initialCluster="$KWM_ETCD_INITIAL_CLUSTER" template service etcd
  )" template service unit)" template util write-file
)
#------------------------------------------------------------------------------#
echo Enabling and restarting service.
#------------------------------------------------------------------------------#
$(name="etcd" template service enable)
" template util remote-script)
TEMPLATE
}
export -f template_resource_etcd-node

template_resource_kubeconfig-manifest() {
cat <<'TEMPLATE'
apiVersion: v1
kind: Config
current-context: default
clusters:
  - cluster:
      certificate-authority: ${basePath}/cluster-ca-cert.pem
      server: https://${apiserver}:6443
    name: ${clusterName}
contexts:
  - context:
      cluster: ${clusterName}
      user: ${user}
    name: default
users:
  - name: ${user}
    user:
      client-certificate: ${basePath}/${name}-cert.pem
      client-key: ${basePath}/${name}-private-key.pem
TEMPLATE
}
export -f template_resource_kubeconfig-manifest

template_resource_pki() {
cat <<'TEMPLATE'
#------------------------------------------------------------------------------#
echo Rendering public key infrastructure.
#------------------------------------------------------------------------------#
mkdir -p $KWM_LOCAL_PKI_PATH
#------------------------------------------------------------------------------#
echo Generating cluster certificate authority.
#------------------------------------------------------------------------------#
$(basePath="$KWM_LOCAL_PKI_PATH" \
  name="cluster" \
  subj="/CN=$KWM_CLUSTER_NAME" template pki init-ca)
#------------------------------------------------------------------------------#
echo Generating private key for cluster admin to apiserver communication.
#------------------------------------------------------------------------------#
$(basePath="$KWM_LOCAL_PKI_PATH" \
  name="cluster-admin-to-apiserver" template pki private-key)
#------------------------------------------------------------------------------#
echo Generating cluster-ca signed certificate for cluster admin to apiserver communication.
#------------------------------------------------------------------------------#
$(basePath="$KWM_LOCAL_PKI_PATH" \
  name="cluster-admin-to-apiserver" \
  subj="/CN=root/O=system:masters" \
  ca="cluster" template pki signed-cert)
#------------------------------------------------------------------------------#
echo Generating etcd certificate authority.
#------------------------------------------------------------------------------#
$(basePath="$KWM_LOCAL_PKI_PATH" name="etcd" subj="/CN=etcd-ca" template pki init-ca)
#------------------------------------------------------------------------------#
echo Generating private key for apiserver to etcd server communication.
#------------------------------------------------------------------------------#
$(basePath="$KWM_LOCAL_PKI_PATH" name="apiserver-to-etcd" template pki private-key)
#------------------------------------------------------------------------------#
echo Generating etcd-ca signed certificate for apiserver to etcd communication.
#------------------------------------------------------------------------------#
$(basePath="$KWM_LOCAL_PKI_PATH" \
  name="apiserver-to-etcd" \
  subj="/CN=etcd" \
  ext="subjectAltName = $KWM_ETCD_CLIENT_SANS" \
  ca="etcd" template pki signed-cert)
#------------------------------------------------------------------------------#
echo Generating public/private keypair for service accounts
#------------------------------------------------------------------------------#
# TODO: get TLS bootstrapping going to eliminate some PKI creation
# https://kubernetes.io/docs/admin/bootstrap-tokens/
$(basePath="$KWM_LOCAL_PKI_PATH" name="service-account" template pki private-key)
$(basePath="$KWM_LOCAL_PKI_PATH" name="service-account" template pki public-key)
TEMPLATE
}
export -f template_resource_pki

template_resource_startup() {
cat <<'TEMPLATE'
#------------------------------------------------------------------------------#
echo Configuring your PKI.
#------------------------------------------------------------------------------#
$SCRIPT_NAME render pki | bash 2>&1 | sed 's/^/[public key infrastructure] /'
#------------------------------------------------------------------------------#
echo Configure your kubectl for cluster admin access.
#------------------------------------------------------------------------------#
$SCRIPT_NAME render cluster-admin | bash 2>&1 | sed 's/^/[getting admin to cluster] /'
#------------------------------------------------------------------------------#
echo Configuring your nodes.
#------------------------------------------------------------------------------#
$SCRIPT_NAME nodes etcd | xargs -n1 $SCRIPT_NAME render etcd-node | bash 2>&1 | sed 's/^/[etcd] /'
$SCRIPT_NAME nodes controlplane | xargs -n1 $SCRIPT_NAME render controlplane-node | bash 2>&1 | sed 's/^/[controlplane] /'
$SCRIPT_NAME nodes worker | xargs -n1 $SCRIPT_NAME render worker-node | bash 2>&1 | sed 's/^/[worker] /'
#------------------------------------------------------------------------------#
echo Installing a container networking plugin.
#------------------------------------------------------------------------------#
mkdir -p manifests/system
$SCRIPT_NAME render cni-manifest > manifests/system/cni.yml
kubectl --context=${KWM_CLUSTER_NAME} apply -f manifests/system/cni.yml | sed 's/^/[container networking] /'
echo Waiting for container networking to start...
#------------------------------------------------------------------------------#
echo HACK bouncing containerd to pick up cni settings from above.
#------------------------------------------------------------------------------#
sleep 30
(($SCRIPT_NAME nodes controlplane && $SCRIPT_NAME nodes worker) | uniq |
  xargs -n1 -I {} bash -c "echo 'sudo systemctl restart containerd' | $SCRIPT_NAME connect {}"
) 2>&1 | sed 's/^/[hack] /'
#------------------------------------------------------------------------------#
echo Installing kube-dns.
#------------------------------------------------------------------------------#
mkdir -p manifests/system
$SCRIPT_NAME render dns-manifest > manifests/system/kube-dns.yml
kubectl --context=${KWM_CLUSTER_NAME} apply -f manifests/system/kube-dns.yml | sed 's/^/[cluster dns] /'
echo Waiting for kube-dns to start...
#------------------------------------------------------------------------------#
echo Confirming the cluster is functional.
#------------------------------------------------------------------------------#
sleep 30
kubectl --context=${KWM_CLUSTER_NAME} get componentstatus
kubectl --context=${KWM_CLUSTER_NAME} get nodes -o wide
kubectl --context=${KWM_CLUSTER_NAME} get pods -o wide --all-namespaces
TEMPLATE
}
export -f template_resource_startup

template_resource_worker-node() {
cat <<'TEMPLATE'
#------------------------------------------------------------------------------#
echo Sending shared PKI to $KWM_HOSTNAME.
#------------------------------------------------------------------------------#
$(files="cluster-ca-cert.pem cluster-ca-private-key.pem" \
  exec="$KWM_CONNECT" \
  basePath="$KWM_CONFIG_PATH" \
  sourcePath="$KWM_LOCAL_PKI_PATH" template util tar-copy
)
#------------------------------------------------------------------------------#
echo Installing $KWM_HOSTNAME.
#------------------------------------------------------------------------------#
$(exec="$KWM_CONNECT" content="
#------------------------------------------------------------------------------#
echo Generating private key for kublet.
#------------------------------------------------------------------------------#
$(basePath="$KWM_CONFIG_PATH" name="kubelet" template pki private-key)
#------------------------------------------------------------------------------#
echo Generating private key for kube-router.
#------------------------------------------------------------------------------#
$(basePath="$KWM_CONFIG_PATH" name="kube-router" template pki private-key)
#------------------------------------------------------------------------------#
echo Generating cluster-ca signed certificate for kubelet to apiserver communication.
#------------------------------------------------------------------------------#
$(name="kubelet" \
  basePath="$KWM_CONFIG_PATH" \
  subj="/CN=system:node:$KWM_HOSTNAME/O=system:nodes" \
  ext="subjectAltName = IP:$KWM_PRIVATE_IP,DNS:$KWM_HOSTNAME" \
  ca="cluster" template pki signed-cert)
#------------------------------------------------------------------------------#
echo Generating cluster-ca signed certificate for kube-router to apiserver communication.
#------------------------------------------------------------------------------#
$(name="kube-router" \
  basePath="$KWM_CONFIG_PATH" \
  subj="/CN=kube-router" \
  ca="cluster" template pki signed-cert)
#------------------------------------------------------------------------------#
echo Ensuring hostname and loopback reference are set.
#------------------------------------------------------------------------------#
$(name="$KWM_HOSTNAME" template util set-hostname)
#------------------------------------------------------------------------------#
echo Generating CNI network configuration.
#------------------------------------------------------------------------------#
$(content='{ "cniVersion": "0.3.1", "type": "loopback" }' \
  path="/etc/cni/net.d/99-loopback.conf" template util write-file
)
$(content='{"name": "kubernetes", "type": "bridge", "bridge": "kube-bridge", "ipam": { "type": "host-local" }, "isDefaultGateway": true }' \
  path="/etc/cni/net.d/10-kuberouter.conf" template util write-file
)
#------------------------------------------------------------------------------#
echo Installing cri-containerd.
#------------------------------------------------------------------------------#
$(version="$KWM_VERSION_CRI_CONTAINERD" template installer cri-containerd)
$(name="containerd" template service enable)
$(name="cri-containerd" template service enable)
#------------------------------------------------------------------------------#
echo Installing container networking plugins at version $KWM_VERSION_CNI_PLUGIN.
#------------------------------------------------------------------------------#
$(version="$KWM_VERSION_CNI_PLUGIN" template installer cni-plugins)
#------------------------------------------------------------------------------#
echo Generating kubelet config.
#------------------------------------------------------------------------------#
$(path="/etc/kubernetes/kubelet.kubeconfig" \
  content="$(
    name="kubelet" \
    user="system:node:$KWM_HOSTNAME" \
    clusterName="$KWM_CLUSTER_NAME" \
    apiserver="$KWM_APISERVER_PRIVATE_IP" \
    basePath="$KWM_CONFIG_PATH" template resource kubeconfig-manifest
  )" template util write-file
)
#------------------------------------------------------------------------------#
echo Generating kube-router config.
#------------------------------------------------------------------------------#
$(path="/etc/kubernetes/kube-router.kubeconfig" \
  content="$(
    name="kube-router" \
    user="kube-router" \
    clusterName="$KWM_CLUSTER_NAME" \
    apiserver="$KWM_APISERVER_PRIVATE_IP" \
    basePath="$KWM_CONFIG_PATH" template resource kubeconfig-manifest
  )" template util write-file
)
#------------------------------------------------------------------------------#
echo Installing kubelet at version $KWM_VERSION_KUBERNETES.
#------------------------------------------------------------------------------#
$(name="kubelet" version="$KWM_VERSION_KUBERNETES" template installer k8s)
#------------------------------------------------------------------------------#
echo Generating kubelet service.
#------------------------------------------------------------------------------#
$(path="/etc/systemd/system/kubelet.service" content="
  $(name="kubelet" \
    after="cri-containerd.service" \
    requires="cri-containerd.service" \
    roles=$KWM_ROLE \
    exec="$(
      privateIp="$KWM_PRIVATE_IP" \
      clusterDns="$KWM_DNS_SERVICE_IP" \
      podCidr="$KWM_POD_CIDR" template service kubelet
    )" template service unit
)" template util write-file)
#------------------------------------------------------------------------------#
echo Enabling and restarting service.
#------------------------------------------------------------------------------#
$(name="kubelet" template service enable)
#------------------------------------------------------------------------------#
echo Installing socat to power kubectl proxy.
#------------------------------------------------------------------------------#
$(template installer socat)

" template util remote-script)
TEMPLATE
}
export -f template_resource_worker-node

template_service_enable() {
cat <<'TEMPLATE'
systemctl daemon-reload
systemctl enable ${name}
systemctl restart ${name}
TEMPLATE
}
export -f template_service_enable

template_service_etcd() {
cat <<'TEMPLATE'
/usr/local/bin/etcd \\
  --name ${name} \\
  --cert-file=/etc/kubernetes/apiserver-to-etcd-cert.pem \\
  --key-file=/etc/kubernetes/apiserver-to-etcd-private-key.pem \\
  --peer-cert-file=/etc/kubernetes/etcd-to-etcd-cert.pem \\
  --peer-key-file=/etc/kubernetes/etcd-to-etcd-private-key.pem \\
  --trusted-ca-file=/etc/kubernetes/etcd-ca-cert.pem \\
  --peer-trusted-ca-file=/etc/kubernetes/etcd-ca-cert.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${privateIp}:2380 \\
  --listen-peer-urls https://${privateIp}:2380 \\
  --listen-client-urls https://${privateIp}:2379,http://127.0.0.1:2379 \\
  --advertise-client-urls https://${privateIp}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster ${initialCluster} \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
TEMPLATE
}
export -f template_service_etcd

template_service_kube-apiserver() {
cat <<'TEMPLATE'
/usr/local/bin/kube-apiserver \\
  --admission-control=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --advertise-address=${privateIp} \\
  --allow-privileged=true \\
  --apiserver-count=${count} \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/etc/kubernetes/cluster-ca-cert.pem \\
  --etcd-cafile=/etc/kubernetes/etcd-ca-cert.pem \\
  --etcd-certfile=/etc/kubernetes/apiserver-to-etcd-cert.pem \\
  --etcd-keyfile=/etc/kubernetes/apiserver-to-etcd-private-key.pem \\
  --etcd-servers=${etcdServers} \\
  --event-ttl=1h \\
  --insecure-bind-address=127.0.0.1 \\
  --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP \\
  --kubelet-certificate-authority=/etc/kubernetes/cluster-ca-cert.pem \\
  --kubelet-client-certificate=/etc/kubernetes/apiserver-to-kubelet-cert.pem \\
  --kubelet-client-key=/etc/kubernetes/apiserver-to-kubelet-private-key.pem \\
  --kubelet-https=true \\
  --runtime-config=api/all \\
  --service-account-key-file=/etc/kubernetes/service-account-public-key.pem \\
  --service-cluster-ip-range=${serviceCidr} \\
  --service-node-port-range=30000-32767 \\
  --tls-ca-file=/etc/kubernetes/cluster-ca-cert.pem \\
  --tls-cert-file=/etc/kubernetes/client-to-apiserver-cert.pem \\
  --tls-private-key-file=/etc/kubernetes/client-to-apiserver-private-key.pem \\
  --v=2
TEMPLATE
}
export -f template_service_kube-apiserver

template_service_kube-controller-manager() {
cat <<'TEMPLATE'
/usr/local/bin/kube-controller-manager \\
  --address=0.0.0.0 \\
  --allocate-node-cidrs=true \\
  --cluster-cidr=${podCidr} \\
  --cluster-name=${clusterName} \\
  --cluster-signing-cert-file=/etc/kubernetes/cluster-ca-cert.pem \\
  --cluster-signing-key-file=/etc/kubernetes/cluster-ca-private-key.pem \\
  --leader-elect=true \\
  --master=http://127.0.0.1:8080 \\
  --root-ca-file=/etc/kubernetes/cluster-ca-cert.pem \\
  --service-account-private-key-file=/etc/kubernetes/service-account-private-key.pem \\
  --service-cluster-ip-range=${serviceCidr} \\
  --v=2
TEMPLATE
}
export -f template_service_kube-controller-manager

template_service_kube-scheduler() {
cat <<'TEMPLATE'
/usr/local/bin/kube-scheduler \\
  --leader-elect=true \\
  --master=http://127.0.0.1:8080 \\
  --v=2
TEMPLATE
}
export -f template_service_kube-scheduler

template_service_kubelet() {
cat <<'TEMPLATE'
/usr/local/bin/kubelet \\
  --node-ip=${privateIp} \\
  --allow-privileged=true \\
  --anonymous-auth=false \\
  --authorization-mode=Webhook \\
  --client-ca-file=/etc/kubernetes/cluster-ca-cert.pem \\
  --cluster-dns=${clusterDns} \\
  --cluster-domain=cluster.local \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/cri-containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\
  --network-plugin=cni \\
  --cni-bin-dir=/opt/cni/bin \\
  --cni-conf-dir=/etc/cni/net.d \\
  --pod-cidr=${podCidr} \\
  --register-node=true \\
  --runtime-request-timeout=15m \\
  --tls-cert-file=/etc/kubernetes/kubelet-cert.pem \\
  --tls-private-key-file=/etc/kubernetes/kubelet-private-key.pem \\
  --fail-swap-on=false \\
  --v=2 $(
output=""
for role in $roles;
  do output+=",node-role.kubernetes.io/$role=true";
done
[[ -n $output ]] && output="--node-labels=\"${output:1}\""
[[ $roles == *"controlplane"* && $roles != *"worker"* ]] && output+=" --register-with-taints=\"node-role.kubernetes.io/controlplane=true:NoSchedule\""
[[ -n $output ]] && echo "$output"
)
TEMPLATE
}
export -f template_service_kubelet

template_service_unit() {
cat <<'TEMPLATE'
[Unit]
Description=${name}
After=${after}
Requires=${requires}

[Service]
ExecStart=${exec}
Restart=always
RestartSec=3
StartLimitInterval=0

[Install]
WantedBy=multi-user.target
TEMPLATE
}
export -f template_service_unit

template_usage_connect() {
cat <<'TEMPLATE'
Connect to a node being managed by Kubernetes Without Magic.

Usage:
  $SCRIPT_NAME connect <nodeKey>

Examples:
  # Restart kublet on a specified node.
  echo "systemctl status kubelet" | $SCRIPT_NAME connect <nodeKey>

Notes:
  Try running "$SCRIPT_NAME nodes" to see which are available.
TEMPLATE
}
export -f template_usage_connect

template_usage_define() {
cat <<'TEMPLATE'
Display a detailed description of what an environment variable is for.

Usage:
  $SCRIPT_NAME define <variable>

Notes:
  The following variables are available to be defined:
$(sed 's/^/  /' <<<"${vars:-None}")
TEMPLATE
}
export -f template_usage_define

template_usage_env() {
cat <<'TEMPLATE'
Display environment variables needed to generate a script or manifest.

Usage:
  $SCRIPT_NAME env [cluster-admin cni dns pki startup]
  $SCRIPT_NAME env [controlplane-node etcd-node worker-node] <nodeKey>

Examples:
  Use this command to familiarize yourself with what input is needed to
  generate scripts and manifests.

  When a nodeKey is specified, KWM looks for KWM_[var]_[nodeKey] environment
  variables and reassigns them to the same name without the node key. For
  example, KWM_ROLE_nodekey=test becomes KWM_ROLE=test for the duration of
  the command.

  This can be illustrated by the following commands:

    KWM_ROLE_node0=worker $SCRIPT_NAME env worker-node node0
    KWM_ROLE_node0=worker $SCRIPT_NAME env worker-node node1

  Note how the value in KWM_ROLE_test is collapsed into the value KWM_ROLE
  when there is a corresponding entry for KWM_ROLE_[nodeKey].
TEMPLATE
}
export -f template_usage_env

template_usage_help() {
cat <<'TEMPLATE'
Welcome to Kubernetes Without Magic!

This project aims to automate the creation and maintenance of production-quality
Kubernetes clusters in the most transparent way possible: using bash scripts you
can inspect before you run.

A familiarity with UNIX fundamentals and systems administration is recommended
to fully utilize this tool. If you're unsure you have the experience needed, try
running through a few workshops:

  $SCRIPT_NAME workshop

When you are ready, prepare an Ubuntu server that you can SSH to with root
access (using any means you wish) and run this:

  $SCRIPT_NAME startup

Unless you have already configured your environment, the first response will be
an error. Don't worry, that's expected.

Read on and you'll have a cluster running in no time.
TEMPLATE
}
export -f template_usage_help

template_usage_install() {
cat <<'TEMPLATE'
Script not in PATH, run this to configure your shell:
  . <("${BASE_PATH%%/}/$SCRIPT_NAME" install)
TEMPLATE
}
export -f template_usage_install

template_usage_main() {
cat <<'TEMPLATE'
Welcome to Kubernetes Without Magic! ($VERSION)

Usage:
  $SCRIPT_NAME <command>

Available Commands:
  connect    connect to a kwm-managed node
  define     show the definition for an environment variable
  env        show environment variables needed to render a script or manifest
  nodes      show nodes present in your environment
  render     generate a script or manifest from environment variables
  startup    generate a script to bootstrap a cluster in one command
  unset      unset all KWM_.* environment variables
  version    show current version
  workshop   learn the fundamental skills required to use this tool

Script Location:
  ${BASE_PATH%%/}

Notes:
  Please run "$SCRIPT_NAME help" if you are just getting started.
TEMPLATE
}
export -f template_usage_main

template_usage_nodes() {
cat <<'TEMPLATE'
List all nodes of a given type which are present in your environment.

Usage:
  $SCRIPT_NAME nodes [all etcd controlplane worker]

Examples:
  # Quick hack illustrating how this command finds nodes.
  KWM_ROLE_one=etcd KWM_ROLE_two="controlplane worker" $SCRIPT_NAME nodes all
  KWM_ROLE_one=etcd KWM_ROLE_two="controlplane worker" $SCRIPT_NAME nodes etcd
  KWM_ROLE_one=etcd KWM_ROLE_two="controlplane worker" $SCRIPT_NAME nodes worker
  KWM_ROLE_one=etcd KWM_ROLE_two="controlplane worker" $SCRIPT_NAME nodes controlplane

  # Display etcd install script for every etcd node.
  $SCRIPT_NAME nodes etcd | xargs -n 1 $SCRIPT_NAME render etcd-node

  # Execute etcd install script for every etcd node.
  $SCRIPT_NAME nodes etcd | xargs -n 1 -P 5 $SCRIPT_NAME render etcd-node | bash
TEMPLATE
}
export -f template_usage_nodes

template_usage_render() {
cat <<'TEMPLATE'
Render a template for bootstrapping a Kubernetes cluster.

Usage:
  $SCRIPT_NAME render [cluster-admin pki startup cni-manifest dns-manifest]
  $SCRIPT_NAME render [controlplane-node etcd-node worker-node] <nodeKey>

Examples:
  # Inspect script to generate public key infrastructure.
  $SCRIPT_NAME render pki

  # Run script to generate public key infrastructure.
  $SCRIPT_NAME render pki | bash

  # Save script to generate public key infrastructure (to share with others).
  $SCRIPT_NAME render pki > generate-pki

  # Inspect yaml manifest to install container networking interface.
  $SCRIPT_NAME render cni-manifest

  # Save yaml manifest to install container networking interface.
  $SCRIPT_NAME render cni-manifest > cni.yml

  # Install container networking solution in your cluster.
  kubectl -f cni.yml
TEMPLATE
}
export -f template_usage_render

template_usage_unset() {
cat <<'TEMPLATE'
Unset all environment variables that begin with KWM_.

Usage:
  . <($SCRIPT_NAME unset)

Examples:
  # Show see what commands will be run.
  $SCRIPT_NAME unset | cat
TEMPLATE
}
export -f template_usage_unset

template_usage_workshop() {
cat <<'TEMPLATE'
Run workshops to confirm you are ready to use Kubernetes Without Magic.

  $SCRIPT_NAME workshop environment-variables | less

  TODO:
  // $SCRIPT_NAME workshop streams
  // $SCRIPT_NAME workshop redirection
  // $SCRIPT_NAME workshop networking
  // $SCRIPT_NAME workshop pki
  // ...

Notes:
  It is recommended that you open a second terminal for running the exercises
  in these workshops.

  You can navigate the text of the workshop using the arrow keys. Typing the
  letter q will exit. For more information, try running "man less".
TEMPLATE
}
export -f template_usage_workshop

template_util_remote-script() {
cat <<'TEMPLATE'
${exec} sudo -s <<"REMOTE_SCRIPT"
${content}
REMOTE_SCRIPT
TEMPLATE
}
export -f template_util_remote-script

template_util_set-hostname() {
cat <<'TEMPLATE'
sudo grep -q -F "${name}" /etc/hosts || (printf "%s\n" "127.0.0.1 ${name}" | sudo tee -a /etc/hosts)
sudo hostname ${name}
TEMPLATE
}
export -f template_util_set-hostname

template_util_tar-copy() {
cat <<'TEMPLATE'
tar c -C ${sourcePath} ${files} | ${exec} "(sudo mkdir -p ${basePath}; cd ${basePath}; sudo tar xf -)"
TEMPLATE
}
export -f template_util_tar-copy

template_util_write-file() {
cat <<'TEMPLATE'
mkdir -p $(dirname ${path})
cat <<"WRITE_FILE" > ${path}
${content}
WRITE_FILE
TEMPLATE
}
export -f template_util_write-file

template_workshop_environment-variables() {
cat <<'TEMPLATE'
$(header "# Show all variables in your current environment.")
printenv

$(header "# Display one variable using printenv.")
printenv HOME

$(header "# Display one variable using echo.")
echo \$HOME

$(header "# Set the value of MESSAGE for the duration of one command")
MESSAGE=hello printenv MESSAGE

$(header "# Show that MESSAGE is no longer set")
echo \$MESSAGE

$(header "# Export MESSAGE, making it available to all commands in the current session")
export MESSAGE=hello

$(header "# Confirm that MESSAGE is still available.")
echo \$MESSAGE

$(header "# Open a new terminal session and confirm that MESSAGE was not persisted to it.")
echo \$MESSAGE

$(header "# Save environment variables to a file.")
cat > settings <<EOF
export SETTING_ONE=one
export SETTING_TWO=two
EOF

$(header "# "Source" the file you created into your shell.")
. ./settings

$(header "# Confirm the variables are now available in your environment")
echo \$SETTING_ONE
echo \$SETTING_TWO

$(header "# Unset a variable, removing it from your environment")
unset \$SETTING_ONE
echo \$SETTING_ONE

$(header "# Now try this with KWM.")
$(header "# It can tell you what environment variables are needed to render commands")
$(header "# for your cluster. Let's see what is required to generate your public key")
$(header "# infrastructure:")
$SCRIPT_NAME env pki

$(header "# You should see that some values are missing. Try supplying one")
KWM_CLUSTER_NAME=workshop $SCRIPT_NAME env pki

$(header "# Now provide all of them, and render the commands")
export KWM_CLUSTER_NAME=workshop
export KWM_APISERVER_PUBLIC_IP=55.55.55.55
export KWM_ETCD_CLIENT_SANS=IP:55.55.55.55

$(header "# Now render and inspect the script")
$SCRIPT_NAME render pki

$(header "# If it looks good, run it!")
$SCRIPT_NAME render pki | bash
TEMPLATE
}
export -f template_workshop_environment-variables


# Kick off KWM, removing any command line flags from the argument listing.
main $(sed 's/ --ignore-missing-env//g' <<< "$@")
